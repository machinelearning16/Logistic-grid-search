{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Dec 24 12:21:30 2017\n",
    "\n",
    "@author: Luca\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve,f1_score,auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "def dataCleaning(df, features_to_delete):\n",
    "    for feature in features_to_delete:\n",
    "        df.drop(feature, axis=1, inplace=True)\n",
    "        \n",
    "    df = df.replace(np.nan, \"\")\n",
    "    df = df.replace(\"NaN\", \"\")\n",
    "    df['was_liked'] = np.where(df['was_liked'] == \"FALSE\",0,1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def checkTicketsDistribution(df):\n",
    "    num_true = len(df.loc[df[\"was_liked\"] == 1])\n",
    "    num_false = len(df.loc[df[\"was_liked\"] == 0])\n",
    "    print(\"Number of True cases: {0} ({1:2.2f}%)\".format(num_true, (num_true/(num_true+num_false))*100))\n",
    "    print(\"Number of False cases: {0} ({1:2.2f}%)\".format(num_false, (num_false/(num_true+num_false))*100))\n",
    "\n",
    "    \n",
    "def splitData(df, X, y):\n",
    "    split_test_size = 0.20\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = split_test_size, random_state = 42)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def checkColumnsTypes(df):\n",
    "    print(\"Data types and their frequency: \\n{}\".format(df.dtypes.value_counts()))\n",
    "    print(\"\")\n",
    "    object_columns_df = df.select_dtypes(include=['object'])\n",
    "    print(object_columns_df.iloc[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originial shape is: \n",
      " (9998, 23)\n",
      "\n",
      "\n",
      "Shape after cleaning: \n",
      " (9998, 6)\n",
      "\n",
      "Number of True cases: 5214 (52.15%)\n",
      "Number of False cases: 4784 (47.85%)\n",
      "-------------------FITTING AND TRANSFORMING TRAINING SET-------------------\n",
      "(9998, 5)\n",
      "(9998, 21220)\n",
      "80.00% in training set\n",
      "20.00% in test set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"DATASET10000.csv\"\n",
    "features_to_delete = [\"index\",\"URL\", \"Date-GMT\",\"Comments\",\"Likes\",\n",
    "                        \"Location_ID\", \"Location_Name\",\n",
    "                        \"lat\", \"lng\", \"Original_Photo\",\n",
    "                        \"Standard_Resolution_Photo_640\", \"Low_Resolution_Photo_320\", \"Thumbnail_Photo_150\",\n",
    "                        \"username\",\"Full_Name\",\"User_ID\",\"Profile_Picture\"]\n",
    "\n",
    "# FROM CSV TO DATAFRAME\n",
    "df = pd.read_csv(file, low_memory=False, encoding=\"ISO-8859-1\", dtype='unicode')\n",
    "\n",
    "# DATAFRAME SIZE\n",
    "print(\"Originial shape is: \\n\", df.shape)\n",
    "print(\"\")\n",
    "\n",
    "# DATAFRAME CLEANING\n",
    "df = dataCleaning(df,features_to_delete)\n",
    "print(\"\")\n",
    "print(\"Shape after cleaning: \\n\", df.shape)\n",
    "print(\"\")\n",
    "\n",
    "# DATAFRAME DISTRIBUTION CHECK\n",
    "checkTicketsDistribution(df)\n",
    "\n",
    "# RELEVANT FEATURES\n",
    "#features = [\"created_time\", \"caption_text\", \"hashtags\", \"users_in_photo\",\"items\", \"num_of_followers\"]\n",
    "feature_to_predict = [\"was_liked\"]\n",
    "features = [\"Date\",\"Caption\",\"Hashtags\",\"num_of_followers\",\"items\"]\n",
    "\n",
    "# CONVERSION TO LISTS\n",
    "X = df[features].values\n",
    "y = df[feature_to_predict].values\n",
    "\n",
    "# COUNTVECTORIZER\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "#TRAINING SET\n",
    "print(\"-------------------FITTING AND TRANSFORMING TRAINING SET-------------------\")\n",
    "#to_fit_transform = [\"caption_text\",\"hashtags\",\"users_in_photo\",\"items\"]\n",
    "to_fit_transform = [\"Caption\",\"Hashtags\",\"items\"]\n",
    "\n",
    "total_data = df\n",
    "total_data = total_data.drop(['was_liked'],axis=1) # Dropped label column \n",
    "\n",
    "for feature in to_fit_transform:\n",
    "    total_data[feature] = list((vect.fit_transform(total_data[feature])))\n",
    "    total_data[feature] = total_data[feature].astype(str)  # Converting every compressed metric data to hashable string format\n",
    "\n",
    "total_data = total_data.replace(np.nan,0)\n",
    "print(total_data.shape)\n",
    "df_encoded = pd.get_dummies(total_data, columns=to_fit_transform, drop_first=True)\n",
    "print(df_encoded.shape)\n",
    "#     e = open('columnames.pickle', 'wb')\n",
    "#     pickle.dump(list(df_encoded.columns), e)\n",
    "#     e.close()\n",
    "X_with_numerical_cols = df_encoded.values  # X with all cols converted into numeric format\n",
    "X_train, y_train, X_test, y_test = splitData(df, X_with_numerical_cols, y)  # splitting data with all new numeric cols\n",
    "print(\"{0:0.2f}% in training set\".format((len(X_train)/len(df.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(X_test)/len(df.index))*100))\n",
    "print(\"\")\n",
    "\n",
    "YTrain = pd.DataFrame(y_train,columns=feature_to_predict)\n",
    "YTest = pd.DataFrame(y_test,columns=feature_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Number of True cases: 4161 (52.03%)\n",
      "Number of False cases: 3837 (47.97%)\n",
      "\n",
      "Testing Set:\n",
      "Number of True cases: 1053 (52.65%)\n",
      "Number of False cases: 947 (47.35%)\n",
      "\n",
      "-------------------FINISHED FITTING AND TRANSFORMING TRAINING SET-------------------\n",
      "-------------------TRAINING SET-------------------\n",
      "(7998, 21220)\n",
      "(7998, 1)\n",
      "-------------------TESTING SET-------------------\n",
      "(2000, 21220)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "#DATAFRAME DISTRIBUTION CHECK AFTER SPLITTING\n",
    "print(\"Training Set:\")\n",
    "checkTicketsDistribution(YTrain)\n",
    "print(\"\")\n",
    "print(\"Testing Set:\")\n",
    "checkTicketsDistribution(YTest)\n",
    "print(\"\")\n",
    "\n",
    "print(\"-------------------FINISHED FITTING AND TRANSFORMING TRAINING SET-------------------\")\n",
    "\n",
    "#CHECK SETS SHAPE\n",
    "print(\"-------------------TRAINING SET-------------------\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"-------------------TESTING SET-------------------\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Parameters of knn from here and put it in knn and then check accuracy of knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# load the diabetes datasets\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "tuned_parameters = [dict(n_neighbors = [4,5,6,7],\n",
    "                         weights = [\"uniform\", \"distance\"],\n",
    "                        algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                        p = [1, 2])\n",
    "                   ]\n",
    "\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = KNeighborsClassifier()\n",
    "grid = GridSearchCV(model, tuned_parameters)\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "# print(grid)\n",
    "print(grid.best_params_)\n",
    "# summarize the results of the grid search\n",
    "# print(grid.best_score_)\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      " READY FOR TRAINING \n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      " FITITNG TRAINING SET FOR THE MODEL \n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      " PREDICTING... \n",
      "-----------------------------------------------\n",
      "[['1512441198' '26402652' 0 ..., 0 0 0]\n",
      " ['1512110104' '22111608' 0 ..., 0 0 0]\n",
      " ['1513965946' '83161316' 0 ..., 0 0 0]\n",
      " ..., \n",
      " ['1511833588' '26402652' 0 ..., 0 0 0]\n",
      " ['1514133631' '86354080' 0 ..., 0 0 0]\n",
      " ['1508943992' '24198445' 0 ..., 0 0 0]]\n",
      "[0 0 1 ..., 0 1 1]\n",
      "[0 0 1 ..., 0 1 1]\n",
      "Accuracy: \n",
      "0.721\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      "[[603 344]\n",
      " [214 839]]\n",
      "\n",
      "AUC 0.71675937709\n",
      "F1 Score:  0.750447227191\n"
     ]
    }
   ],
   "source": [
    "#MODEL TRAINING\n",
    "print(\"-----------------------------------------------\\n READY FOR TRAINING \\n-----------------------------------------------\")\n",
    "#LOGISTIC REGRESSION CLASSIFIER\n",
    "#lr = lm.LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "#          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "#          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    " #         verbose=0, warm_start=False)\n",
    "\n",
    "#NAIVE BAYES CLASSIFIER\n",
    "#lr = GaussianNB()\n",
    "\n",
    "#KNN CLASSIFIER\n",
    "#lr = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "#RANDOM FOREST CLASSIFIER\n",
    "#lr=ensemble.RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "#DECISION TREE CLASSIFIER\n",
    "#lr = tree.DecisionTreeClassifier(max_depth=80)\n",
    "\n",
    "#GRADIENT BOOSTING CLASSIFIER\n",
    "lr = ensemble.GradientBoostingClassifier(n_estimators=50)\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------\\n FITITNG TRAINING SET FOR THE MODEL \\n-----------------------------------------------\")\n",
    "lr.fit(X_train, y_train.ravel())\n",
    "\n",
    "print(\"-----------------------------------------------\\n PREDICTING... \\n-----------------------------------------------\")\n",
    "#y_pred_class = lr.predict_proba(X_test)\n",
    "y_pred_class_bin = lr.predict(X_test)\n",
    "print(X_test)\n",
    "#print(y_pred_class)\n",
    "print(y_pred_class_bin)\n",
    "print(y_test.ravel())\n",
    "\n",
    "#TESTING TOOLS\n",
    "print(\"Accuracy: \")\n",
    "print(metrics.accuracy_score(y_test, y_pred_class_bin))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Confusion Matrix: \\n\")\n",
    "print(metrics.confusion_matrix(y_test,y_pred_class_bin))\n",
    "print(\"\")\n",
    "\n",
    "#false_positive_rate, true_positive_rate, _ = roc_curve(y_test,y_pred_class[:,1],pos_label=\"TRUE\")\n",
    "false_positive_rate, true_positive_rate, _ = roc_curve(y_test,y_pred_class_bin)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "print(\"AUC\", roc_auc)\n",
    "\n",
    "F1 = f1_score(y_test, y_pred_class_bin, average='binary')\n",
    "print(\"F1 Score: \", F1)\n",
    "    \n",
    "# #DUMPING TO FILES\n",
    "# dill.dump(lr, open(\"model.p\", \"wb\"))\n",
    "# dill.dump(vect, open(\"countvectorizer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: \n",
    "0.6055\n",
    "\n",
    "Confusion Matrix: \n",
    "\n",
    "[[703 244]\n",
    " [545 508]]\n",
    "\n",
    "AUC 0.612387697041\n",
    "F1 Score:  0.562880886427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
